{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU4nXdqSf8mv",
        "outputId": "de598857-695b-4011-84da-5427a218b6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RT81RvFf8pA",
        "outputId": "a33a911d-2488-4694-abde-e5107f3b8bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 timm-0.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjLllQKWf8r9",
        "outputId": "8f387bb4-fb97-4448-e9d6-35fe369c02b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.32.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-metric-learning\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSaphwdBf8ul",
        "outputId": "b9de861c-a309-4cde-db25-3aff5f848ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-metric-learning\n",
            "  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->pytorch-metric-learning) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->pytorch-metric-learning) (16.0.6)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Installing collected packages: pytorch-metric-learning\n",
            "Successfully installed pytorch-metric-learning-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "\n",
        "# DATALOADER\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# BUILDING MODEL\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models  # Import ResNet-50\n",
        "\n",
        "# TRAINING\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "\n",
        "# OTHER STUFF\n",
        "import timm\n",
        "from transformers import (get_linear_schedule_with_warmup,\n",
        "                          get_constant_schedule,\n",
        "                          get_cosine_schedule_with_warmup,\n",
        "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
        "                          get_constant_schedule_with_warmup)\n",
        "import gc\n",
        "import transformers\n",
        "from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n",
        "from pytorch_metric_learning import losses\n",
        "#import open_clip\n",
        "\n",
        "# UTILS\n",
        "#import utilities\n",
        "\n",
        "# Load the ResNet-50 model\n",
        "resnet50 = models.resnet50(pretrained=True)"
      ],
      "metadata": {
        "id": "K02AaJDef8xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a59bd6-1525-4a85-97b2-fa9ddf5a1a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 146MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import faiss\n",
        "import copy\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    r\"\"\"Implement of large margin arc distance: :\n",
        "        Args:\n",
        "            in_features: size of each input sample\n",
        "            out_features: size of each output sample\n",
        "            s: norm of input feature\n",
        "            m: margin\n",
        "            cos(theta + m)\n",
        "        \"\"\"\n",
        "    def __init__(self, in_features, out_features, s=30.0,\n",
        "                 m=0.50, easy_margin=False, ls_eps=0.0, device=torch.device('cuda')):\n",
        "        super(ArcMarginProduct, self).__init__()\n",
        "        self.device = device\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps  # label smoothing\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        # --------------------------- convert label to one-hot ---------------------\n",
        "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
        "        one_hot = torch.zeros(cosine.size(), device=self.device)\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "\n",
        "        return output\n",
        "\n",
        "class DenseCrossEntropy(nn.Module):\n",
        "    def forward(self, x, target):\n",
        "        x = x.float()\n",
        "        target = target.float()\n",
        "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
        "\n",
        "        loss = -logprobs * target\n",
        "        loss = loss.sum(-1)\n",
        "        return loss.mean()\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        x = x.float()\n",
        "        target = target.float()\n",
        "        probs = torch.nn.functional.softmax(x, dim=-1)\n",
        "        logprobs = torch.log(probs)\n",
        "\n",
        "        loss = -logprobs * target * (1 - probs) ** self.gamma\n",
        "        loss = loss.sum(-1)\n",
        "        return loss.mean()\n",
        "\n",
        "class ArcMarginProduct_subcenter(nn.Module):\n",
        "    def __init__(self, in_features, out_features, k=3):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n",
        "        self.reset_parameters()\n",
        "        self.k = k\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, features):\n",
        "        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n",
        "        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n",
        "        cosine, _ = torch.max(cosine_all, dim=2)\n",
        "        return cosine\n",
        "\n",
        "class ArcFaceLossAdaptiveMargin(nn.modules.Module):\n",
        "    def __init__(self, margins, s=30.0, crit='ce'):\n",
        "        super().__init__()\n",
        "        if crit == 'ce':\n",
        "            self.crit = DenseCrossEntropy()\n",
        "        else:\n",
        "            self.crit = FocalLoss()\n",
        "        self.s = s\n",
        "        self.margins = margins\n",
        "\n",
        "    def forward(self, logits, labels, out_dim):\n",
        "        ms = []\n",
        "        ms = self.margins[labels.cpu().numpy()]\n",
        "        cos_m = torch.from_numpy(np.cos(ms)).float().cuda()\n",
        "        sin_m = torch.from_numpy(np.sin(ms)).float().cuda()\n",
        "        th = torch.from_numpy(np.cos(math.pi - ms)).float().cuda()\n",
        "        mm = torch.from_numpy(np.sin(math.pi - ms) * ms).float().cuda()\n",
        "        labels = F.one_hot(labels, out_dim).float()\n",
        "        logits = logits.float()\n",
        "        cosine = logits\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * cos_m.view(-1,1) - sine * sin_m.view(-1,1)\n",
        "        phi = torch.where(cosine > th.view(-1,1), phi, cosine - mm.view(-1,1))\n",
        "        output = (labels * phi) + ((1.0 - labels) * cosine)\n",
        "        output *= self.s\n",
        "        loss = self.crit(output, labels)\n",
        "        return loss\n",
        "\n",
        "def set_seed(seed):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "\n",
        "def get_similiarity_hnsw(embeddings_gallery, emmbeddings_query, k):\n",
        "    # this is guy is really fast\n",
        "    print('Processing indices...')\n",
        "\n",
        "    s = time.time()\n",
        "    index = faiss.IndexHNSWFlat(embeddings_gallery.shape[1], 32)\n",
        "    index.add(embeddings_gallery)\n",
        "\n",
        "    scores, indices = index.search(emmbeddings_query, k)\n",
        "    e = time.time()\n",
        "\n",
        "    print(f'Finished processing indices, took {e - s}s')\n",
        "    return scores, indices\n",
        "\n",
        "def get_similiarity_l2(embeddings_gallery, emmbeddings_query, k):\n",
        "    print('Processing indices...')\n",
        "\n",
        "    s = time.time()\n",
        "    index = faiss.IndexFlatL2(embeddings_gallery.shape[1])\n",
        "    index.add(embeddings_gallery)\n",
        "\n",
        "    scores, indices = index.search(emmbeddings_query, k)\n",
        "    e = time.time()\n",
        "\n",
        "    print(f'Finished processing indices, took {e - s}s')\n",
        "    return scores, indices\n",
        "\n",
        "\n",
        "def get_similiarity_IP(embeddings_gallery, emmbeddings_query, k):\n",
        "    print('Processing indices...')\n",
        "\n",
        "    s = time.time()\n",
        "    index = faiss.IndexFlatIP(embeddings_gallery.shape[1])\n",
        "    index.add(embeddings_gallery)\n",
        "\n",
        "    scores, indices = index.search(emmbeddings_query, k)\n",
        "    e = time.time()\n",
        "\n",
        "    print(f'Finished processing indices, took {e - s}s')\n",
        "    return scores, indices\n",
        "\n",
        "def get_similiarity(embeddings, k):\n",
        "    print('Processing indices...')\n",
        "\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "\n",
        "    res = faiss.StandardGpuResources()\n",
        "\n",
        "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
        "\n",
        "    index.add(embeddings)\n",
        "\n",
        "    scores, indices = index.search(embeddings, k)\n",
        "    print('Finished processing indices')\n",
        "\n",
        "    return scores, indices\n",
        "\n",
        "def map_per_image(label, predictions, k=5):\n",
        "    try:\n",
        "        return 1 / (predictions[:k].index(label) + 1)\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "\n",
        "def map_per_set(labels, predictions, k=5):\n",
        "    return np.mean([map_per_image(l, p, k) for l,p in zip(labels, predictions)])\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, window_size=None):\n",
        "        self.length = 0\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def reset(self):\n",
        "        self.length = 0\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        if self.window_size and (self.count >= self.window_size):\n",
        "            self.reset()\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def get_lr_groups(param_groups):\n",
        "        groups = sorted(set([param_g['lr'] for param_g in param_groups]))\n",
        "        groups = [\"{:2e}\".format(group) for group in groups]\n",
        "        return groups\n",
        "\n",
        "def convert_indices_to_labels(indices, labels):\n",
        "    indices_copy = copy.deepcopy(indices)\n",
        "    for row in indices_copy:\n",
        "        for j in range(len(row)):\n",
        "            row[j] = labels[row[j]]\n",
        "    return indices_copy\n",
        "\n",
        "class Multisample_Dropout(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.1):\n",
        "        super(Multisample_Dropout, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout((i+1)*.1) for i in range(5)])\n",
        "\n",
        "    def forward(self, x, module):\n",
        "        x = self.dropout(x)\n",
        "        return torch.mean(torch.stack([module(dropout(x)) for dropout in self.dropouts],dim=0),dim=0)\n",
        "\n",
        "def transforms_auto_augment(image_path, image_size):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    train_transforms = transforms.Compose([transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET), transforms.PILToTensor()])\n",
        "    return train_transforms(image)\n",
        "\n",
        "def transforms_cutout(image_path, image_size):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
        "    train_transforms = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ImageCompression(quality_lower=99, quality_upper=100),\n",
        "            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
        "            A.Resize(image_size, image_size),\n",
        "            A.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    return train_transforms(image=image)['image']\n",
        "\n",
        "def transforms_happy_whale(image_path, image_size):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
        "    aug8p3 = A.OneOf([\n",
        "            A.Sharpen(p=0.3),\n",
        "            A.ToGray(p=0.3),\n",
        "            A.CLAHE(p=0.3),\n",
        "        ], p=0.5)\n",
        "\n",
        "    train_transforms = A.Compose([\n",
        "            A.ShiftScaleRotate(rotate_limit=15, scale_limit=0.1, border_mode=cv2.BORDER_REFLECT, p=0.5),\n",
        "            A.Resize(image_size, image_size),\n",
        "            aug8p3,\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    return train_transforms(image=image)['image']\n",
        "\n",
        "def transforms_valid(image_path, image_size):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    valid_transforms = transforms.Compose([transforms.PILToTensor()])\n",
        "    return valid_transforms(image)"
      ],
      "metadata": {
        "id": "LABS-dTcRXA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    model_name = 'ResNet-50'  # Change the model name\n",
        "    model_data = ''  # Not needed for ResNet-50\n",
        "    samples_per_class = 50\n",
        "    n_classes = 0  # Update this based on your dataset\n",
        "    min_samples = 4\n",
        "    image_size = 224\n",
        "    hidden_layer = 1024  # This parameter is not used for ResNet-50\n",
        "    seed = 5\n",
        "    workers = 12\n",
        "    train_batch_size = 8\n",
        "    valid_batch_size = 32\n",
        "    emb_size = 512  # Not used for ResNet-50\n",
        "    learning_rate = 3e-4  # Adjusted for ResNet-50\n",
        "    weight_decay = 1e-5  # Adjusted for ResNet-50\n",
        "    autocast = True\n",
        "    n_warmup_steps = 1000\n",
        "    n_epochs = 10\n",
        "    device = torch.device('cuda')\n",
        "    s = 30.\n",
        "    m = .45\n",
        "    m_min = .05\n",
        "    acc_steps = 4\n",
        "    global_step = 0\n",
        "    reduce_lr = 0.1\n",
        "    crit = 'ce'\n"
      ],
      "metadata": {
        "id": "L5nGuDSVWQJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load extensions and set device\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS6C5ZMYXwZZ",
        "outputId": "59a224de-b8a8-4fde-b2ce-36fd0b47c163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "VV1btZl9aMF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# used for training\n",
        "training_samples = []\n",
        "values_counts = []\n",
        "num_classes = 0"
      ],
      "metadata": {
        "id": "OVfI1ssRYDpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LDCBU6Ssd_rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ba233d-11c8-4188-850f-d0ed8ab2374b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "df_g = df.groupby('class', group_keys=True).apply(lambda x: x)\n",
        "\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "train_df['path'] = train_df.apply(lambda x: '/content/drive/MyDrive/train' + '/' + x['name'], axis=1)\n",
        "\n",
        "\n",
        "# remove 9397815.jpg from the list!\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/test_kaggletest.csv')\n",
        "test_df = test_df.drop(test_df[test_df.name == '9397815.jpg'].index) # smt wrong with this img\n",
        "test_df['path'] = test_df.apply(lambda x: '/content/drive/MyDrive/test' + '/' + x['name'], axis=1)\n",
        "\n",
        "df = pd.concat([\n",
        "    test_df[['class','path']],\n",
        "    train_df[['class', 'path']]\n",
        "])\n",
        "df_g = df.groupby('class', group_keys=True).apply(lambda x: x)\n"
      ],
      "metadata": {
        "id": "huW7WJQnYDuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for group in tqdm(set(df_g['class'])):\n",
        "    names = list(df_g.path[df_g['class'] == group])\n",
        "    if len(names) >= CFG.min_samples:\n",
        "        paths = [\n",
        "            name for name in names[:CFG.samples_per_class]\n",
        "        ]\n",
        "\n",
        "        values_counts.append(len(paths))\n",
        "        training_samples.extend([\n",
        "            (p, num_classes) for p in paths\n",
        "        ])\n",
        "\n",
        "        num_classes += 1"
      ],
      "metadata": {
        "id": "eUFqTolhYDxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7d8acd-4ce3-4923-8890-4aa35407700d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9691/9691 [00:12<00:00, 773.25it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = training_samples\n",
        "value_counts = np.array(values_counts)\n",
        "CFG.n_classes = num_classes"
      ],
      "metadata": {
        "id": "lTYafMIRYDzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_train), CFG.n_classes"
      ],
      "metadata": {
        "id": "O360aS8oYD2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c23d60e-a2fd-45ca-c331-c436f8a6e7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(196944, 9691)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resnet50 Model"
      ],
      "metadata": {
        "id": "uQrwjbm5a0pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, hidden_size, n_classes, k=3):\n",
        "        super(Head, self).__init__()\n",
        "        self.fc = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.fc(x)\n",
        "        return output, F.normalize(x)\n",
        "\n",
        "class HeadV2(nn.Module):\n",
        "    def __init__(self, hidden_size, n_classes, k=3):\n",
        "        super(HeadV2, self).__init__()\n",
        "        self.fc = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.fc(x)\n",
        "        return output, F.normalize(x)\n",
        "\n",
        "class HeadV3(nn.Module):\n",
        "    def __init__(self, hidden_size, n_classes, k=3):\n",
        "        super(HeadV3, self).__init__()\n",
        "        self.fc = nn.Linear(hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.fc(x)\n",
        "        return output, F.normalize(x)\n"
      ],
      "metadata": {
        "id": "kwVOwsTrYD4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, head_size, version='v1', k=3):\n",
        "        super(Model, self).__init__()\n",
        "        if version == 'v1':\n",
        "            self.head = Head(head_size, k)\n",
        "        elif version == 'v2':\n",
        "            self.head = HeadV2(head_size, k)\n",
        "        elif version == 'v3':\n",
        "            self.head = HeadV3(head_size, k)\n",
        "        else:\n",
        "            self.head = Head(head_size, k)\n",
        "\n",
        "        # Use ResNet-50 as the backbone\n",
        "        resnet_backbone = models.resnet50(pretrained=True)\n",
        "        self.encoder = nn.Sequential(*list(resnet_backbone.children())[:-2])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.mean([2, 3])  # Global average pooling\n",
        "        return self.head(x)\n",
        "\n",
        "    def get_parameters(self):\n",
        "        parameter_settings = []\n",
        "        parameter_settings.extend(\n",
        "            self.get_parameter_section(\n",
        "                [(n, p) for n, p in self.encoder.named_parameters()],\n",
        "                lr=CFG.resnet_lr,  # You need to define this learning rate\n",
        "                wd=CFG.resnet_wd  # You need to define this weight decay\n",
        "            )\n",
        "        )\n",
        "\n",
        "        parameter_settings.extend(\n",
        "            self.get_parameter_section(\n",
        "                [(n, p) for n, p in self.head.named_parameters()],\n",
        "                lr=CFG.hd_lr,\n",
        "                wd=CFG.hd_wd\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return parameter_settings\n",
        "\n",
        "\n",
        "\n",
        "    def get_parameter_section(self, parameters, lr=None, wd=None):\n",
        "      parameter_settings = []\n",
        "\n",
        "      lr_is_dict = isinstance(lr, dict)\n",
        "      wd_is_dict = isinstance(wd, dict)\n",
        "\n",
        "      for no, (n, p) in enumerate(parameters):\n",
        "        if lr_is_dict:\n",
        "            temp_lr = next((v for k, v in sorted(lr.items()) if int(k) > no), lr[str(len(lr) - 1)])\n",
        "        else:\n",
        "            temp_lr = lr\n",
        "\n",
        "        if wd_is_dict:\n",
        "            temp_wd = next((v for k, v in sorted(wd.items()) if int(k) > no), wd[str(len(wd) - 1)])\n",
        "        else:\n",
        "            temp_wd = wd\n",
        "\n",
        "        weight_decay = 0.0 if 'bias' in n else temp_wd\n",
        "\n",
        "        parameter_setting = {\"params\": p, \"lr\": temp_lr, \"weight_decay\": weight_decay}\n",
        "\n",
        "        parameter_settings.append(parameter_setting)\n",
        "\n",
        "        # print(f'no {no} | params {n} | lr {temp_lr} | weight_decay {weight_decay} | requires_grad {p.requires_grad}')\n",
        "        return parameter_settings\n"
      ],
      "metadata": {
        "id": "EvhtsHahYD6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ArcFace_criterion(logits_m, target, margins):\n",
        "    arc = ArcFaceLossAdaptiveMargin(margins=margins, s=CFG.s, crit=CFG.crit)\n",
        "    loss_m = arc(logits_m, target, CFG.n_classes)\n",
        "    return loss_m"
      ],
      "metadata": {
        "id": "XBXJsGrqYD9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Validation"
      ],
      "metadata": {
        "id": "wOOoG1CsEb4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, scaler, scheduler, epoch):\n",
        "    model.train()\n",
        "    loss_metrics = AverageMeter()\n",
        "    criterion = ArcFace_criterion  # You should define this function as mentioned earlier\n",
        "\n",
        "    tmp = np.sqrt(1 / np.sqrt(value_counts))  # You need to define value_counts\n",
        "    margins = (tmp - tmp.min()) / (tmp.max() - tmp.min()) * CFG.m + CFG.m_min\n",
        "\n",
        "    bar = tqdm(train_loader)\n",
        "    for step, data in enumerate(bar):\n",
        "        step += 1\n",
        "        images = data['images'].to(CFG.device, dtype=torch.float)\n",
        "        labels = data['labels'].to(CFG.device)\n",
        "        batch_size = labels.size(0)\n",
        "\n",
        "        with autocast(enabled=CFG.autocast):\n",
        "            outputs = model(images)  # ResNet-50 model doesn't output features\n",
        "\n",
        "        loss = criterion(outputs, labels, margins)\n",
        "        loss_metrics.update(loss.item(), batch_size)\n",
        "        loss = loss / CFG.acc_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if step % CFG.acc_steps == 0 or step == len(bar):\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "            CFG.global_step += 1\n",
        "\n",
        "        lrs = get_lr_groups(optimizer.param_groups)\n",
        "\n",
        "        loss_avg = loss_metrics.avg\n",
        "\n",
        "        bar.set_postfix(loss=loss_avg, epoch=epoch, lrs=lrs, step=CFG.global_step)\n",
        "\n",
        "@torch.no_grad()\n",
        "def val(model, valid_loader):\n",
        "    model.eval()\n",
        "\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "\n",
        "    for data in tqdm(valid_loader):\n",
        "        images = data['images'].to(CFG.device, dtype=torch.float)\n",
        "        labels = data['labels'].to(CFG.device)\n",
        "\n",
        "        embeddings = model(images)  # ResNet-50 model doesn't output features\n",
        "\n",
        "        all_embeddings.append(embeddings.detach().cpu().numpy())\n",
        "        all_labels.append(labels.detach().cpu().numpy())\n",
        "\n",
        "    all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    return all_embeddings, all_labels"
      ],
      "metadata": {
        "id": "9Q-LcrobYEAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(train_loader,\n",
        "             gallery_loader,\n",
        "             query_loader,\n",
        "             experiment_folder,\n",
        "             version='v1',\n",
        "             k=3,\n",
        "             reduce_lr_on_epoch=1,\n",
        "             use_rampup=True):\n",
        "\n",
        "    os.makedirs(experiment_folder, exist_ok=True)\n",
        "\n",
        "    # Initialize ResNet-50 Backbone\n",
        "    resnet_backbone = models.resnet50(pretrained=True)\n",
        "    in_features = resnet_backbone.fc.in_features\n",
        "\n",
        "# Create your Model using the ResNet-50 backbone\n",
        "    model = Model(in_features, version, k).to(CFG.device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.get_parameters())\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.autocast)\n",
        "\n",
        "    steps_per_epoch = math.ceil(len(train_loader) / CFG.acc_steps)\n",
        "\n",
        "    num_training_steps = math.ceil(CFG.n_epochs * steps_per_epoch)\n",
        "\n",
        "    if use_rampup:\n",
        "        scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
        "                                                    num_training_steps=num_training_steps,\n",
        "                                                    num_warmup_steps=CFG.n_warmup_steps)\n",
        "    else:\n",
        "        scheduler = get_constant_schedule(optimizer)\n",
        "\n",
        "    best_score = 0\n",
        "    best_updated_ = 0\n",
        "    CFG.global_step = 0\n",
        "    for epoch in range(math.ceil(CFG.n_epochs)):\n",
        "        print(f'starting epoch {epoch}')\n",
        "\n",
        "\n",
        "        # train of product-10k\n",
        "        train(model, train_loader, optimizer, scaler, scheduler, epoch)\n",
        "\n",
        "        # aicrowd test data\n",
        "        print('gallery embeddings')\n",
        "        embeddings_gallery, labels_gallery = val(model, gallery_loader)\n",
        "        print('query embeddings')\n",
        "        embeddings_query, labels_query = val(model, query_loader)\n",
        "\n",
        "        # idk why it is needed\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # calculate validation score\n",
        "        _, indices = utilities.get_similiarity_l2(embeddings_gallery, embeddings_query, 1000)\n",
        "\n",
        "\n",
        "        indices = indices.tolist()\n",
        "        labels_gallery = labels_gallery.tolist()\n",
        "        labels_query = labels_query.tolist()\n",
        "\n",
        "        preds = convert_indices_to_labels(indices, labels_gallery)\n",
        "        score = map_per_set(labels_query, preds)\n",
        "        print('validation score', score)\n",
        "\n",
        "        # save model\n",
        "        torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                }, f'{experiment_folder}/model_epoch_{epoch+1}_mAP3_{score:.2f}.pt')\n",
        "\n",
        "        # early stopping\n",
        "        if score > best_score:\n",
        "            best_updated_ = 0\n",
        "            best_score = score\n",
        "\n",
        "        best_updated_ += 1\n",
        "\n",
        "        if best_updated_ >= 3:\n",
        "            print('no improvement done training....')\n",
        "            break\n",
        "\n",
        "        if (epoch + 1) % reduce_lr_on_epoch == 0:\n",
        "            scheduler.base_lrs = [g['lr'] * CFG.reduce_lr for g in optimizer.param_groups]\n",
        "\n",
        "        # to speed up the training\n",
        "        if epoch > 3:\n",
        "            break"
      ],
      "metadata": {
        "id": "HlMKzUwzYECv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loaders for Training and Validation"
      ],
      "metadata": {
        "id": "viZKVpf1V8bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_img(img_path, is_gray=False):\n",
        "    mode = cv2.IMREAD_COLOR if not is_gray else cv2.IMREAD_GRAYSCALE\n",
        "    img = cv2.imread(img_path, mode)\n",
        "    if not is_gray:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "def get_final_transform():\n",
        "    final_transform = T.Compose([\n",
        "            T.Resize(\n",
        "                size=(CFG.image_size, CFG.image_size),\n",
        "                interpolation=T.InterpolationMode.BICUBIC,\n",
        "                antialias=True),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(\n",
        "                mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                std=(0.26862954, 0.26130258, 0.27577711)\n",
        "            )\n",
        "        ])\n",
        "    return final_transform"
      ],
      "metadata": {
        "id": "JbNRmuoOYEFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProductDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 data,\n",
        "                 transform=None,\n",
        "                 final_transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.final_transform = final_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img = read_img(self.data[idx][0])\n",
        "\n",
        "        if self.transform is not None:\n",
        "            if isinstance(self.transform, A.Compose):\n",
        "                img = self.transform(image=img)['image']\n",
        "            else:\n",
        "                img = self.transform(img)\n",
        "\n",
        "        if self.final_transform is not None:\n",
        "            if isinstance(img, np.ndarray):\n",
        "                img =  Image.fromarray(img)\n",
        "            img = self.final_transform(img)\n",
        "\n",
        "        product_id = self.data[idx][1]\n",
        "        return {\"images\": img, \"labels\": product_id}\n"
      ],
      "metadata": {
        "id": "WJil6lG0YEHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_product_10k_dataloader(data_train, data_aug='image_net'):\n",
        "\n",
        "    transform = None\n",
        "    if data_aug == 'image_net':\n",
        "        transform = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.AutoAugment(T.AutoAugmentPolicy.IMAGENET)\n",
        "        ])\n",
        "\n",
        "    elif data_aug == 'aug_mix':\n",
        "        transform = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.AugMix()\n",
        "        ])\n",
        "    elif data_aug == 'happy_whale':\n",
        "        aug8p3 = A.OneOf([\n",
        "            A.Sharpen(p=0.3),\n",
        "            A.ToGray(p=0.3),\n",
        "            A.CLAHE(p=0.3),\n",
        "        ], p=0.5)\n",
        "\n",
        "        transform = A.Compose([\n",
        "            A.ShiftScaleRotate(rotate_limit=15, scale_limit=0.1, border_mode=cv2.BORDER_REFLECT, p=0.5),\n",
        "            A.Resize(CFG.image_size, CFG.image_size),\n",
        "            aug8p3,\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)\n",
        "        ])\n",
        "\n",
        "    elif data_aug == 'cut_out':\n",
        "        transform = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ImageCompression(quality_lower=99, quality_upper=100),\n",
        "            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
        "            A.Resize(CFG.image_size, CFG.image_size),\n",
        "            A.Cutout(max_h_size=int(CFG.image_size * 0.4),\n",
        "                     max_w_size=int(CFG.image_size * 0.4),\n",
        "                     num_holes=1, p=0.5),\n",
        "        ])\n",
        "    elif data_aug == 'clip':\n",
        "        transform = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.RandomResizedCrop(\n",
        "                size=(224, 224),\n",
        "                scale=(0.9, 1.0),\n",
        "                ratio=(0.75, 1.3333),\n",
        "                interpolation=T.InterpolationMode.BICUBIC,\n",
        "                antialias=True\n",
        "            )\n",
        "        ])\n",
        "    elif data_aug == 'clip+image_net':\n",
        "        transform = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n",
        "            T.RandomResizedCrop(\n",
        "                size=(224, 224),\n",
        "                scale=(0.9, 1.0),\n",
        "                ratio=(0.75, 1.3333),\n",
        "                interpolation=T.InterpolationMode.BICUBIC,\n",
        "                antialias=True\n",
        "            )\n",
        "        ])\n",
        "\n",
        "    final_transform = get_final_transform()\n",
        "    train_dataset = ProductDataset(data_train,\n",
        "                                   transform,\n",
        "                                   final_transform)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size = CFG.train_batch_size,\n",
        "                              num_workers=CFG.workers,\n",
        "                              shuffle=True,\n",
        "                              drop_last=True)\n",
        "    print(f'Training Data -> Dataset Length ({len(train_dataset)})')\n",
        "    return train_loader"
      ],
      "metadata": {
        "id": "3Chl4TzsYEKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aicrowd_data_loader(csv_path, img_dir='/content/drive/MyDrive/testing-dataset/development_test_data/development_test_data'):\n",
        "    df_g = pd.read_csv(csv_path)\n",
        "    df_g_ = df_g[['img_path', 'product_id']]\n",
        "    df_g_['img_path'] = df_g_.apply(lambda x: img_dir + '/' + x['img_path'], axis=1)\n",
        "    data_ = np.array(df_g_).tolist()\n",
        "\n",
        "    final_transform = get_final_transform()\n",
        "    dataset = ProductDataset(data_, None, final_transform)\n",
        "    data_loader = DataLoader(dataset,\n",
        "                             batch_size = CFG.valid_batch_size,\n",
        "                             num_workers=CFG.workers,\n",
        "                             shuffle=False,\n",
        "                             drop_last=False)\n",
        "\n",
        "    print(f'{csv_path} -> Dataset Length ({len(dataset)})')\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "VIExr0aEYENK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gallery_loader = aicrowd_data_loader('/content/drive/MyDrive/testing-dataset/development_test_data/development_test_data/gallery.csv')\n",
        "query_loader = aicrowd_data_loader('/content/drive/MyDrive/testing-dataset/development_test_data/development_test_data/queries.csv')"
      ],
      "metadata": {
        "id": "mfiHAzapYEPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "604632ef-6f90-4bd9-ff7d-789f3bab2c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-935150d57e21>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_g_['img_path'] = df_g_.apply(lambda x: img_dir + '/' + x['img_path'], axis=1)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/testing-dataset/development_test_data/development_test_data/gallery.csv -> Dataset Length (1067)\n",
            "/content/drive/MyDrive/testing-dataset/development_test_data/development_test_data/queries.csv -> Dataset Length (1935)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-935150d57e21>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_g_['img_path'] = df_g_.apply(lambda x: img_dir + '/' + x['img_path'], axis=1)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "sN_-oqK1X_2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "version = 'v2'\n",
        "data_aug = 'image_net'\n",
        "CFG.reduce_lr = 0.1\n",
        "train_loader = get_product_10k_dataloader(data_train, data_aug)\n",
        "#experiment_folder = f'my_experiments/{CFG.model_name}-{CFG.model_data}-{str(data_aug)}-{str(version)}-p10k-h&m-Arcface(k={str(k)})-All-Epoch({str(CFG.n_epochs)})-Reduce_LR_0.1'\n",
        "#training(train_loader,\n",
        "        # gallery_loader,\n",
        "         #query_loader,\n",
        "         #experiment_folder,\n",
        "         #version=version,\n",
        "         #k=k)\n",
        "# idk why it is needed\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "MyE38ypuYESp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1316d4-9327-4344-ff9c-fdba21a8db58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data -> Dataset Length (196944)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zD3NP0pYYEVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_3N3vriYEYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYx7Q7OGYEar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cTnazi3_YEda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lti_zKcfYEgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KIVfWkocYEjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ezd_gscoYElo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQ2lDBzYYEog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QOCrfrkkYEq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8VjGWpjYEto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AurB-Yx8YEv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4LqUD3IVYEzM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}